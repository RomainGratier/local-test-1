# Data Engineering Challenge: E-Commerce Analytics Pipeline

## Challenge Overview
You are tasked with building a robust data pipeline for an e-commerce platform that processes millions of daily transactions. The pipeline must integrate data from multiple sources, perform real-time analytics, and maintain data quality standards.

## Scenario
TechMart, a growing e-commerce company, needs a scalable data solution to:
- Process real-time transaction data from their web application
- Integrate with external payment and shipping APIs
- Generate daily analytics reports for business stakeholders
- Maintain a data warehouse for historical analysis
- Ensure data quality and handle failures gracefully

## Technology Stack
- **Primary Database**: PostgreSQL (transactional data)
- **Data Warehouse**: Google BigQuery (analytics and reporting)
- **Cloud Platform**: Google Cloud Platform (GCP)
- **Languages**: Python (preferred) or SQL
- **Additional Tools**: Docker, Apache Airflow (optional)

## Challenge Duration
- **Time Limit**: 4 hours
- **Deliverables**: Complete pipeline code, documentation, and test results

## Success Criteria
1. âœ… Data pipeline processes all sample data correctly
2. âœ… BigQuery tables are properly structured and optimized
3. âœ… PostgreSQL schema supports ACID transactions
4. âœ… Error handling and data validation implemented
5. âœ… Performance meets specified benchmarks
6. âœ… Code is production-ready with proper documentation

## Getting Started
1. Review the requirements in `REQUIREMENTS.md`
2. Examine sample data in the `data/` directory
3. Set up your development environment
4. Implement the solution following the specifications
5. Run tests and validate your implementation

Good luck! ðŸš€